#!/usr/bin/env node

import fetch from 'node-fetch';
import dotenv from 'dotenv';
import fs from 'fs';

console.log('üß™ Testando diferentes modelos e configura√ß√µes da OpenAI...\n');

// Carregar API key
const envConfig = dotenv.parse(fs.readFileSync('.env.local'));
const OPENAI_API_KEY = envConfig.OPENAI_API_KEY;

async function testModel(model, maxTokens = 100) {
  console.log(`\nüìù Testando modelo: ${model} (max_tokens: ${maxTokens})`);
  
  const startTime = Date.now();
  
  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: model,
        messages: [
          { role: 'system', content: 'Voc√™ √© um assistente.' },
          { role: 'user', content: 'Responda apenas: OK' }
        ],
        temperature: 0.7,
        max_tokens: maxTokens
      })
    });
    
    const responseTime = Date.now() - startTime;
    
    if (response.ok) {
      const data = await response.json();
      console.log(`   ‚úÖ Sucesso em ${responseTime}ms`);
      console.log(`   Resposta: ${data.choices[0].message.content}`);
      console.log(`   Tokens usados: ${data.usage?.total_tokens || 'N/A'}`);
    } else {
      const error = await response.json();
      console.log(`   ‚ùå Erro ${response.status}: ${error.error?.message}`);
      
      // Se for erro de modelo n√£o encontrado, pode ser um nome incorreto
      if (error.error?.code === 'model_not_found') {
        console.log(`   üí° Modelo "${model}" n√£o existe ou n√£o est√° dispon√≠vel`);
      }
    }
  } catch (error) {
    console.log(`   ‚ùå Erro de rede: ${error.message}`);
  }
}

async function testLargeRequest() {
  console.log('\nüìù Testando requisi√ß√£o grande (como no response-synthesizer)...');
  
  const largePrompt = `
Voc√™ √© um especialista em urbanismo e regulamenta√ß√µes municipais de Porto Alegre.
Seu papel √© sintetizar informa√ß√µes sobre o Plano Diretor de forma clara e acess√≠vel.

Dados encontrados:
${Array(50).fill('Exemplo de dados do regime urban√≠stico...').join('\n')}

Por favor, sintetize esses dados de forma clara.
  `.trim();
  
  const startTime = Date.now();
  
  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          { role: 'system', content: 'Voc√™ √© um assistente.' },
          { role: 'user', content: largePrompt }
        ],
        temperature: 0.7,
        max_tokens: 8000
      })
    });
    
    const responseTime = Date.now() - startTime;
    
    if (response.ok) {
      const data = await response.json();
      console.log(`   ‚úÖ Sucesso em ${responseTime}ms`);
      console.log(`   Tokens usados: ${data.usage?.total_tokens || 'N/A'}`);
    } else {
      const error = await response.json();
      console.log(`   ‚ùå Erro ${response.status}: ${error.error?.message}`);
    }
  } catch (error) {
    console.log(`   ‚ùå Erro: ${error.message}`);
  }
}

async function checkQuota() {
  console.log('\nüí∞ Verificando informa√ß√µes da conta...');
  
  try {
    // Tentar endpoint de modelos como proxy para verificar se a API est√° acess√≠vel
    const response = await fetch('https://api.openai.com/v1/models', {
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      }
    });
    
    if (response.ok) {
      console.log('   ‚úÖ API acess√≠vel');
      
      // Verificar rate limit headers
      const remaining = response.headers.get('x-ratelimit-remaining-requests');
      const limit = response.headers.get('x-ratelimit-limit-requests');
      
      if (remaining && limit) {
        console.log(`   Rate limit: ${remaining}/${limit} requisi√ß√µes restantes`);
      }
    }
  } catch (error) {
    console.log('   ‚ùå Erro:', error.message);
  }
}

async function main() {
  // Verificar quota primeiro
  await checkQuota();
  
  // Testar diferentes modelos
  await testModel('gpt-3.5-turbo');
  await testModel('gpt-4');
  await testModel('gpt-4-turbo-preview');
  await testModel('gpt-4o-mini'); // Modelo usado no response-synthesizer
  
  // Testar requisi√ß√£o grande
  await testLargeRequest();
  
  console.log('\n\nüìä DIAGN√ìSTICO:');
  console.log('- Se gpt-4o-mini falha mas outros funcionam, use outro modelo');
  console.log('- Se requisi√ß√µes grandes demoram muito, reduza max_tokens');
  console.log('- Se h√° erros de rate limit, aguarde ou use outro provider');
}

main().catch(console.error);