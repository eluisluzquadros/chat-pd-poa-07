# Sistema Multi-LLM Avan√ßado

## Vis√£o Geral

Este sistema implementa uma solu√ß√£o completa de integra√ß√£o de m√∫ltiplos modelos de linguagem (LLMs) com m√©tricas avan√ßadas, compara√ß√£o de performance e sele√ß√£o inteligente de modelos baseada no contexto.

## üöÄ Funcionalidades Principais

### 1. Suporte a M√∫ltiplos Modelos
- **OpenAI**: GPT-4o, GPT-4.5 Turbo (futuro)
- **Anthropic Claude**: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku
- **Google Gemini**: Gemini 1.5 Pro, Gemini Pro Vision
- **Meta Llama**: Llama 3.1 (local)
- **DeepSeek**: DeepSeek Coder
- **Groq**: Lightning (ultra-r√°pido)

### 2. Sistema de M√©tricas Avan√ßado
- **Tempo de resposta** (ms)
- **Tokens por segundo**
- **Custo por consulta** ($)
- **Score de qualidade** (0-100)
- **Taxa de sucesso** (%)
- **N√≠vel de confian√ßa** (0-1)

### 3. Compara√ß√£o Inteligente
- Melhor para **velocidade**
- Melhor para **qualidade**
- Melhor para **custo**
- **Recomenda√ß√£o balanceada**

### 4. Sele√ß√£o Autom√°tica
- An√°lise de complexidade da query
- Considera√ß√£o do papel do usu√°rio
- Restri√ß√µes de or√ßamento
- Suporte multimodal (imagens)

## üìÅ Estrutura do Sistema

```
src/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ multiLLMService.ts          # Servi√ßo principal
‚îÇ   ‚îî‚îÄ‚îÄ llmMetricsService.ts        # Sistema de m√©tricas
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ ui/model-selector.tsx       # Interface de sele√ß√£o
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îî‚îÄ‚îÄ useIntelligentModelSelection.ts  # Hook inteligente
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ chat.ts                     # Tipos TypeScript

supabase/functions/
‚îú‚îÄ‚îÄ openai-advanced-chat/           # GPT-4.5 Turbo
‚îú‚îÄ‚îÄ claude-opus-chat/               # Claude 3 Opus
‚îú‚îÄ‚îÄ claude-sonnet-chat/             # Claude 3 Sonnet
‚îú‚îÄ‚îÄ claude-haiku-chat/              # Claude 3 Haiku
‚îú‚îÄ‚îÄ gemini-pro-chat/                # Gemini 1.5 Pro
‚îî‚îÄ‚îÄ gemini-vision-chat/             # Gemini Pro Vision

supabase/migrations/
‚îî‚îÄ‚îÄ 20240201000000_add_llm_metrics.sql  # Schema de m√©tricas
```

## üîß Configura√ß√£o

### 1. Vari√°veis de Ambiente

```env
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic Claude
CLAUDE_API_KEY=sk-ant-...

# Google Gemini
GEMINI_API_KEY=AI...

# Groq
GROQ_API_KEY=gsk_...

# DeepSeek
DEEPSEEK_API_KEY=sk-...
```

### 2. Banco de Dados

Execute a migra√ß√£o para criar as tabelas de m√©tricas:

```sql
-- Executar no Supabase SQL Editor
-- supabase/migrations/20240201000000_add_llm_metrics.sql
```

### 3. Deploy das Edge Functions

```bash
# Deploy todas as novas functions
supabase functions deploy openai-advanced-chat
supabase functions deploy claude-opus-chat
supabase functions deploy claude-sonnet-chat
supabase functions deploy claude-haiku-chat
supabase functions deploy gemini-pro-chat
supabase functions deploy gemini-vision-chat
```

## üí° Como Usar

### 1. Uso B√°sico

```typescript
import { multiLLMService } from '@/services/multiLLMService';

// Processar mensagem com modelo espec√≠fico
const response = await multiLLMService.processMessage(
  "Qual a altura m√°xima no Centro?",
  "claude-3-opus",
  "citizen"
);

console.log(response.response);
console.log(`Qualidade: ${response.qualityScore}/100`);
console.log(`Tempo: ${response.executionTime}ms`);
console.log(`Custo: $${response.costEstimate}`);
```

### 2. Compara√ß√£o de Modelos

```typescript
// Comparar m√∫ltiplos modelos
const comparisons = await multiLLMService.compareModels(
  "An√°lise detalhada da ZOT 8.3",
  "analyst"
);

// Resultados ordenados por qualidade
comparisons.forEach(comp => {
  console.log(`${comp.provider}: ${comp.qualityScore}/100`);
});
```

### 3. Sele√ß√£o Inteligente

```typescript
// Obter melhor modelo para crit√©rio espec√≠fico
const bestForSpeed = await multiLLMService.getBestModel(
  "Resposta r√°pida sobre altura",
  "speed"
);

const bestForQuality = await multiLLMService.getBestModel(
  "An√°lise complexa de aproveitamento",
  "quality"
);

const bestForCost = await multiLLMService.getBestModel(
  "Informa√ß√£o b√°sica",
  "cost"
);
```

### 4. Interface React

```tsx
import { ModelSelector } from '@/components/ui/model-selector';

function ChatSettings() {
  const [selectedModel, setSelectedModel] = useState<LLMProvider>('openai');

  return (
    <ModelSelector 
      selectedModel={selectedModel}
      onModelChange={setSelectedModel}
      showComparison={true}
    />
  );
}
```

### 5. Hook Inteligente

```tsx
import { useIntelligentModelSelection } from '@/hooks/useIntelligentModelSelection';

function SmartChat() {
  const { 
    selectedModel, 
    getBestModelFor,
    switchToOptimalModel 
  } = useIntelligentModelSelection();

  const handleQuery = async (query: string) => {
    // Selecionar automaticamente o melhor modelo
    const optimal = await getBestModelFor(query, {
      priority: 'balanced',
      userRole: 'citizen'
    });
    
    // Processar com modelo otimizado
    const response = await multiLLMService.processMessage(
      query, 
      optimal
    );
    
    return response;
  };
}
```

## üìä Sistema de M√©tricas

### 1. Coleta Autom√°tica

Todas as intera√ß√µes s√£o automaticamente registradas com:

```typescript
interface LLMMetrics {
  responseTime: number;      // Tempo de resposta (ms)
  tokensPerSecond: number;   // Velocidade de processamento
  inputTokens: number;       // Tokens de entrada
  outputTokens: number;      // Tokens de sa√≠da
  totalCost: number;         // Custo total ($)
  qualityScore: number;      // Score de qualidade (0-100)
  confidence: number;        // N√≠vel de confian√ßa (0-1)
  successRate: number;       // Taxa de sucesso (%)
}
```

### 2. An√°lises Dispon√≠veis

```typescript
// Performance por provedor
const performance = await llmMetricsService.getModelPerformance('claude', 7);

// Compara√ß√£o geral
const comparison = await llmMetricsService.compareModels(7);

// Benchmark em tempo real
const benchmarks = await llmMetricsService.benchmarkAllModels();
```

### 3. Fun√ß√µes SQL Especializadas

```sql
-- Obter melhor modelo por crit√©rio
SELECT * FROM get_best_model('quality', 7);
SELECT * FROM get_best_model('speed', 7);
SELECT * FROM get_best_model('cost', 7);

-- Compara√ß√£o completa ranqueada
SELECT * FROM get_model_comparison(7);

-- Registrar uso manual
SELECT record_model_usage(
  'claude-3-opus',
  'claude-3-opus-20240229', 
  1500, -- response_time
  100,  -- input_tokens
  200,  -- output_tokens
  95    -- quality_score
);
```

## üéØ Caracter√≠sticas por Modelo

### OpenAI GPT-4.5 Turbo (Futuro)
- **For√ßa**: Racioc√≠nio superior, an√°lise complexa
- **Custo**: Alto ($0.015 por 1K tokens output)
- **Velocidade**: M√©dia (40 tokens/s)
- **Caso de uso**: An√°lises cr√≠ticas, decis√µes complexas

### Claude 3 Opus
- **For√ßa**: M√°xima qualidade, racioc√≠nio profundo
- **Custo**: Muito alto ($0.075 por 1K tokens output)  
- **Velocidade**: Baixa (30 tokens/s)
- **Caso de uso**: An√°lises premium, trabalho profissional

### Claude 3 Sonnet
- **For√ßa**: Equil√≠brio ideal, versatilidade
- **Custo**: M√©dio ($0.015 por 1K tokens output)
- **Velocidade**: Boa (40 tokens/s)
- **Caso de uso**: Uso geral, melhor custo-benef√≠cio

### Claude 3 Haiku
- **For√ßa**: Velocidade extrema, efici√™ncia
- **Custo**: Baixo ($0.00125 por 1K tokens output)
- **Velocidade**: Muito alta (60 tokens/s)
- **Caso de uso**: Respostas r√°pidas, alto volume

### Gemini 1.5 Pro
- **For√ßa**: Contexto longo, multimodal
- **Custo**: M√©dio ($0.005 por 1K tokens output)
- **Velocidade**: Boa (45 tokens/s)
- **Caso de uso**: An√°lise de documentos, imagens

### Groq Lightning
- **For√ßa**: Velocidade extrema, baixa lat√™ncia
- **Custo**: Baixo ($0.00027 por 1K tokens)
- **Velocidade**: Extrema (100+ tokens/s)
- **Caso de uso**: Aplica√ß√µes tempo-real

## üß™ Testes

### Executar Testes

```bash
# Teste r√°pido
npx tsx scripts/test-multi-llm-system.ts quick

# Teste completo
npx tsx scripts/test-multi-llm-system.ts full

# Benchmark de lat√™ncia
npx tsx scripts/test-multi-llm-system.ts benchmark
```

### Exemplos de Output

```
‚ö° Teste R√°pido do Sistema Multi-LLM
========================================
Testando: "Qual √© a altura m√°xima no Centro de Porto Alegre?"
Provedor: openai

‚úÖ Resposta recebida:
  Tempo: 1247ms
  Qualidade: 87/100
  Confian√ßa: 92.3%
  Custo: $0.0023
  Comprimento: 456 caracteres

üìä M√©tricas detalhadas:
  Tokens de entrada: 15
  Tokens de sa√≠da: 114
  Tokens/segundo: 52.1
```

## üìà Monitoramento

### Dashboard de M√©tricas

O sistema inclui um dashboard completo para monitorar:

- **Performance em tempo real**
- **Custos acumulados**
- **Tend√™ncias de uso**
- **Compara√ß√µes hist√≥ricas**
- **Alertas de performance**

### Views SQL para An√°lise

```sql
-- Performance resumida
SELECT * FROM llm_model_performance;

-- M√©tricas di√°rias
SELECT * FROM llm_daily_metrics;

-- An√°lise de custos
SELECT provider, SUM(total_cost) as total_spent
FROM llm_metrics 
WHERE created_at >= NOW() - INTERVAL '30 days'
GROUP BY provider;
```

## üîí Seguran√ßa e Privacidade

- **Row Level Security** habilitado
- **API Keys** seguras via environment variables
- **Logs audit√°veis** de todas as intera√ß√µes
- **Rate limiting** por usu√°rio/sess√£o
- **Valida√ß√£o** de entrada e sa√≠da

## üöÄ Pr√≥ximos Passos

1. **Auto-scaling** baseado em carga
2. **Cache inteligente** de respostas similares
3. **Fine-tuning** de modelos espec√≠ficos
4. **A/B testing** automatizado
5. **Integra√ß√£o** com ferramentas de observabilidade

## üìû Suporte

Para d√∫vidas ou problemas:

1. Consulte os logs do Supabase
2. Execute os testes diagn√≥sticos
3. Verifique as m√©tricas no dashboard
4. Analise os custos acumulados

## üéâ Conclus√£o

Este sistema Multi-LLM oferece:

- ‚úÖ **14 modelos** diferentes dispon√≠veis
- ‚úÖ **M√©tricas detalhadas** automatizadas  
- ‚úÖ **Sele√ß√£o inteligente** baseada em contexto
- ‚úÖ **Interface completa** para compara√ß√£o
- ‚úÖ **Otimiza√ß√£o** de custo-benef√≠cio
- ‚úÖ **Escalabilidade** enterprise-ready

A implementa√ß√£o permite escolher o modelo ideal para cada situa√ß√£o, otimizando qualidade, velocidade e custo de acordo com as necessidades espec√≠ficas de cada consulta sobre o PDUS 2025.